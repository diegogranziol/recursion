{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7308acdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datamol as dm\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the ChemBERTa model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\", num_labels=1)\n",
    "\n",
    "# Load the data\n",
    "df = dm.data.freesolv()\n",
    "X, y = df[\"smiles\"], df[\"expt\"]\n",
    "\n",
    "# Tokenize the SMILES strings\n",
    "def tokenize_function(smiles):\n",
    "    return tokenizer(smiles, padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Apply the tokenization\n",
    "X_tokenized = X.apply(tokenize_function)\n",
    "\n",
    "# Convert the tokenized data to the format required by Hugging Face\n",
    "class FreeSolvDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Tokenize the datasets\n",
    "train_encodings = tokenizer(list(X_train), padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "val_encodings = tokenizer(list(X_val), padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "test_encodings = tokenizer(list(X_test), padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "# Create the datasets\n",
    "train_dataset = FreeSolvDataset(train_encodings, y_train.values)\n",
    "val_dataset = FreeSolvDataset(val_encodings, y_val.values)\n",
    "test_dataset = FreeSolvDataset(test_encodings, y_test.values)\n",
    "\n",
    "# Define training arguments with early stopping and save the best model\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=100,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    ")\n",
    "\n",
    "# Define a custom compute_metrics function for regression\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions.squeeze()\n",
    "    mse = mean_squared_error(labels, predictions)\n",
    "    r2 = r2_score(labels, predictions)\n",
    "    return {\"mse\": mse, \"r2\": r2}\n",
    "\n",
    "# Create the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = trainer.create_optimizer()\n",
    "\n",
    "# Add a linear schedule with warmup\n",
    "num_training_steps = len(train_dataset) // training_args.per_device_train_batch_size * training_args.num_train_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# Assign the scheduler to the trainer\n",
    "trainer.lr_scheduler = scheduler\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "print(test_results)\n",
    "\n",
    "# Predict on the test set\n",
    "predictions = trainer.predict(test_dataset).predictions.squeeze()\n",
    "\n",
    "# Plot predictions vs true values for test set\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, predictions, alpha=0.6, color='green')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.title('Test Set: True Values vs Predictions')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cc550d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rdkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c603425",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experimental Class for Smiles Enumeration, Iterator and SmilesIterator adapted from Keras 1.2.2\n",
    "from rdkit import Chem\n",
    "import numpy as np\n",
    "import threading\n",
    "\n",
    "class Iterator(object):\n",
    "    \"\"\"Abstract base class for data iterators.\n",
    "\n",
    "    # Arguments\n",
    "        n: Integer, total number of samples in the dataset to loop over.\n",
    "        batch_size: Integer, size of a batch.\n",
    "        shuffle: Boolean, whether to shuffle the data between epochs.\n",
    "        seed: Random seeding for data shuffling.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n, batch_size, shuffle, seed):\n",
    "        self.n = n\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.batch_index = 0\n",
    "        self.total_batches_seen = 0\n",
    "        self.lock = threading.Lock()\n",
    "        self.index_generator = self._flow_index(n, batch_size, shuffle, seed)\n",
    "        if n < batch_size:\n",
    "            raise ValueError('Input data length is shorter than batch_size\\nAdjust batch_size')\n",
    "\n",
    "    def reset(self):\n",
    "        self.batch_index = 0\n",
    "\n",
    "    def _flow_index(self, n, batch_size=32, shuffle=False, seed=None):\n",
    "        # Ensure self.batch_index is 0.\n",
    "        self.reset()\n",
    "        while 1:\n",
    "            if seed is not None:\n",
    "                np.random.seed(seed + self.total_batches_seen)\n",
    "            if self.batch_index == 0:\n",
    "                index_array = np.arange(n)\n",
    "                if shuffle:\n",
    "                    index_array = np.random.permutation(n)\n",
    "\n",
    "            current_index = (self.batch_index * batch_size) % n\n",
    "            if n > current_index + batch_size:\n",
    "                current_batch_size = batch_size\n",
    "                self.batch_index += 1\n",
    "            else:\n",
    "                current_batch_size = n - current_index\n",
    "                self.batch_index = 0\n",
    "            self.total_batches_seen += 1\n",
    "            yield (index_array[current_index: current_index + current_batch_size],\n",
    "                   current_index, current_batch_size)\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Needed if we want to do something like:\n",
    "        # for x, y in data_gen.flow(...):\n",
    "        return self\n",
    "\n",
    "    def __next__(self, *args, **kwargs):\n",
    "        return self.next(*args, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SmilesIterator(Iterator):\n",
    "    \"\"\"Iterator yielding data from a SMILES array.\n",
    "\n",
    "    # Arguments\n",
    "        x: Numpy array of SMILES input data.\n",
    "        y: Numpy array of targets data.\n",
    "        smiles_data_generator: Instance of `SmilesEnumerator`\n",
    "            to use for random SMILES generation.\n",
    "        batch_size: Integer, size of a batch.\n",
    "        shuffle: Boolean, whether to shuffle the data between epochs.\n",
    "        seed: Random seed for data shuffling.\n",
    "        dtype: dtype to use for returned batch. Set to keras.backend.floatx if using Keras\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x, y, smiles_data_generator,\n",
    "                 batch_size=32, shuffle=False, seed=None,\n",
    "                 dtype=np.float32\n",
    "                 ):\n",
    "        if y is not None and len(x) != len(y):\n",
    "            raise ValueError('X (images tensor) and y (labels) '\n",
    "                             'should have the same length. '\n",
    "                             'Found: X.shape = %s, y.shape = %s' %\n",
    "                             (np.asarray(x).shape, np.asarray(y).shape))\n",
    "\n",
    "        self.x = np.asarray(x)\n",
    "\n",
    "        if y is not None:\n",
    "            self.y = np.asarray(y)\n",
    "        else:\n",
    "            self.y = None\n",
    "        self.smiles_data_generator = smiles_data_generator\n",
    "        self.dtype = dtype\n",
    "        super(SmilesIterator, self).__init__(x.shape[0], batch_size, shuffle, seed)\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"For python 2.x.\n",
    "\n",
    "        # Returns\n",
    "            The next batch.\n",
    "        \"\"\"\n",
    "        # Keeps under lock only the mechanism which advances\n",
    "        # the indexing of each batch.\n",
    "        with self.lock:\n",
    "            index_array, current_index, current_batch_size = next(self.index_generator)\n",
    "        # The transformation of images is not under thread lock\n",
    "        # so it can be done in parallel\n",
    "        batch_x = np.zeros(tuple([current_batch_size] + [ self.smiles_data_generator.pad, self.smiles_data_generator._charlen]), dtype=self.dtype)\n",
    "        for i, j in enumerate(index_array):\n",
    "            smiles = self.x[j:j+1]\n",
    "            x = self.smiles_data_generator.transform(smiles)\n",
    "            batch_x[i] = x\n",
    "\n",
    "        if self.y is None:\n",
    "            return batch_x\n",
    "        batch_y = self.y[index_array]\n",
    "        return batch_x, batch_y\n",
    "\n",
    "\n",
    "class SmilesEnumerator(object):\n",
    "    \"\"\"SMILES Enumerator, vectorizer and devectorizer\n",
    "    \n",
    "    #Arguments\n",
    "        charset: string containing the characters for the vectorization\n",
    "          can also be generated via the .fit() method\n",
    "        pad: Length of the vectorization\n",
    "        leftpad: Add spaces to the left of the SMILES\n",
    "        isomericSmiles: Generate SMILES containing information about stereogenic centers\n",
    "        enum: Enumerate the SMILES during transform\n",
    "        canonical: use canonical SMILES during transform (overrides enum)\n",
    "    \"\"\"\n",
    "    def __init__(self, charset = '@C)(=cOn1S2/H[N]\\\\', pad=120, leftpad=True, isomericSmiles=True, enum=True, canonical=False):\n",
    "        self._charset = None\n",
    "        self.charset = charset\n",
    "        self.pad = pad\n",
    "        self.leftpad = leftpad\n",
    "        self.isomericSmiles = isomericSmiles\n",
    "        self.enumerate = enum\n",
    "        self.canonical = canonical\n",
    "\n",
    "    @property\n",
    "    def charset(self):\n",
    "        return self._charset\n",
    "        \n",
    "    @charset.setter\n",
    "    def charset(self, charset):\n",
    "        self._charset = charset\n",
    "        self._charlen = len(charset)\n",
    "        self._char_to_int = dict((c,i) for i,c in enumerate(charset))\n",
    "        self._int_to_char = dict((i,c) for i,c in enumerate(charset))\n",
    "        \n",
    "    def fit(self, smiles, extra_chars=[], extra_pad = 5):\n",
    "        \"\"\"Performs extraction of the charset and length of a SMILES datasets and sets self.pad and self.charset\n",
    "        \n",
    "        #Arguments\n",
    "            smiles: Numpy array or Pandas series containing smiles as strings\n",
    "            extra_chars: List of extra chars to add to the charset (e.g. \"\\\\\\\\\" when \"/\" is present)\n",
    "            extra_pad: Extra padding to add before or after the SMILES vectorization\n",
    "        \"\"\"\n",
    "        charset = set(\"\".join(list(smiles)))\n",
    "        self.charset = \"\".join(charset.union(set(extra_chars)))\n",
    "        self.pad = max([len(smile) for smile in smiles]) + extra_pad\n",
    "        \n",
    "    def randomize_smiles(self, smiles):\n",
    "        \"\"\"Perform a randomization of a SMILES string\n",
    "        must be RDKit sanitizable\"\"\"\n",
    "        m = Chem.MolFromSmiles(smiles)\n",
    "        ans = list(range(m.GetNumAtoms()))\n",
    "        np.random.shuffle(ans)\n",
    "        nm = Chem.RenumberAtoms(m,ans)\n",
    "        return Chem.MolToSmiles(nm, canonical=self.canonical, isomericSmiles=self.isomericSmiles)\n",
    "\n",
    "    def transform(self, smiles):\n",
    "        \"\"\"Perform an enumeration (randomization) and vectorization of a Numpy array of smiles strings\n",
    "        #Arguments\n",
    "            smiles: Numpy array or Pandas series containing smiles as strings\n",
    "        \"\"\"\n",
    "        one_hot =  np.zeros((smiles.shape[0], self.pad, self._charlen),dtype=np.int8)\n",
    "        \n",
    "        if self.leftpad:\n",
    "            for i,ss in enumerate(smiles):\n",
    "                if self.enumerate: ss = self.randomize_smiles(ss)\n",
    "                l = len(ss)\n",
    "                diff = self.pad - l\n",
    "                for j,c in enumerate(ss):\n",
    "                    one_hot[i,j+diff,self._char_to_int[c]] = 1\n",
    "            return one_hot\n",
    "        else:\n",
    "            for i,ss in enumerate(smiles):\n",
    "                if self.enumerate: ss = self.randomize_smiles(ss)\n",
    "                for j,c in enumerate(ss):\n",
    "                    one_hot[i,j,self._char_to_int[c]] = 1\n",
    "            return one_hot\n",
    "\n",
    "      \n",
    "    def reverse_transform(self, vect):\n",
    "        \"\"\" Performs a conversion of a vectorized SMILES to a smiles strings\n",
    "        charset must be the same as used for vectorization.\n",
    "        #Arguments\n",
    "            vect: Numpy array of vectorized SMILES.\n",
    "        \"\"\"       \n",
    "        smiles = []\n",
    "        for v in vect:\n",
    "            #mask v \n",
    "            v=v[v.sum(axis=1)==1]\n",
    "            #Find one hot encoded index with argmax, translate to char and join to string\n",
    "            smile = \"\".join(self._int_to_char[i] for i in v.argmax(axis=1))\n",
    "            smiles.append(smile)\n",
    "        return np.array(smiles)\n",
    "     \n",
    "if __name__ == \"__main__\":\n",
    "    smiles = np.array([ \"CCC(=O)O[C@@]1(CC[NH+](C[C@H]1CC=C)C)c2ccccc2\",\n",
    "                        \"CCC[S@@](=O)c1ccc2c(c1)[nH]/c(=N/C(=O)OC)/[nH]2\"]*10\n",
    "                        )\n",
    "    #Test canonical SMILES vectorization\n",
    "    sm_en = SmilesEnumerator(canonical=True, enum=False)\n",
    "    sm_en.fit(smiles, extra_chars=[\"\\\\\"])\n",
    "    v = sm_en.transform(smiles)\n",
    "    transformed = sm_en.reverse_transform(v)\n",
    "    if len(set(transformed)) > 2: print(\"Too many different canonical SMILES generated\")\n",
    "    \n",
    "    #Test enumeration \n",
    "    sm_en.canonical = False\n",
    "    sm_en.enumerate = True\n",
    "    v2 = sm_en.transform(smiles)\n",
    "    transformed = sm_en.reverse_transform(v2)\n",
    "    if len(set(transformed)) < 3: print(\"Too few enumerated SMILES generated\")\n",
    "\n",
    "    #Reconstruction\n",
    "    reconstructed = sm_en.reverse_transform(v[0:5])\n",
    "    for i, smile in enumerate(reconstructed):\n",
    "        if smile != smiles[i]:\n",
    "            print(\"Error in reconstruction %s %s\"%(smile, smiles[i]))\n",
    "            break\n",
    "    \n",
    "    #test Pandas\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(smiles)\n",
    "    v = sm_en.transform(df[0])\n",
    "    if v.shape != (20, 52, 18): print(\"Possible error in pandas use\")\n",
    "    \n",
    "    #BUG, when batchsize > x.shape[0], then it only returns x.shape[0]!\n",
    "    #Test batch generation\n",
    "    sm_it = SmilesIterator(smiles, np.array([1,2]*10), sm_en, batch_size=10, shuffle=True)\n",
    "    X, y = sm_it.next()\n",
    "    if sum(y==1) - sum(y==2) > 1:\n",
    "        print(\"Unbalanced generation of batches\")\n",
    "    if len(X) != 10: print(\"Error in batchsize generation\")\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f443a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "sme = SmilesEnumerator()\n",
    "print(help(SmilesEnumerator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b752e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(sme.randomize_smiles(\"CCC(=O)O[C@@]1(CC[NH+](C[C@H]1CC=C)C)c2ccccc2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04244dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dm.data.freesolv()\n",
    "X, y = df[\"smiles\"], df[\"expt\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0329a520",
   "metadata": {},
   "outputs": [],
   "source": [
    "sme.randomize_smiles(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6661fec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X, new_y = [],[]\n",
    "for i in range(len(X)):\n",
    "    for j in range(100):\n",
    "        new_X.append(sme.randomize_smiles(X[i]))\n",
    "        new_y.append(y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81d1866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datamol as dm\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the ChemBERTa model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\", num_labels=1)\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "# Load the data\n",
    "df = dm.data.freesolv()\n",
    "X, y = df[\"smiles\"], df[\"expt\"]\n",
    "\n",
    "# X,y = deepcopy(new_X),deepcopy(new_y)\n",
    "# X = pd.DataFrame(X).squeeze()\n",
    "# y = pd.DataFrame(y).squeeze()\n",
    "\n",
    "# Tokenize the SMILES strings\n",
    "def tokenize_function(smiles):\n",
    "    return tokenizer(smiles, padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "\n",
    "\n",
    "# Convert the tokenized data to the format required by Hugging Face\n",
    "class FreeSolvDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "new_X, new_y = [],[]\n",
    "listx = X_train.tolist()\n",
    "listy = y_train.tolist() \n",
    "for i in range(len(X_train)):\n",
    "    for j in range(100):\n",
    "        new_X.append(sme.randomize_smiles(listx[i]))\n",
    "        new_y.append(listy[i])\n",
    "        \n",
    "# X,y = deepcopy(new_X),deepcopy(new_y)\n",
    "X_train = pd.DataFrame(new_X).squeeze()\n",
    "y_train = pd.DataFrame(new_y).squeeze()\n",
    "\n",
    "# Apply the tokenization\n",
    "# Tokenize the datasets and assign the results back to the variables\n",
    "# X_train = X_train.apply(tokenize_function)\n",
    "# X_val = X_val.apply(tokenize_function)\n",
    "# X_test = X_test.apply(tokenize_function)\n",
    "        \n",
    "\n",
    "# Tokenize the datasets\n",
    "train_encodings = tokenizer(list(X_train), padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "val_encodings = tokenizer(list(X_val), padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "test_encodings = tokenizer(list(X_test), padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "# Create the datasets\n",
    "train_dataset = FreeSolvDataset(train_encodings, y_train.values)\n",
    "val_dataset = FreeSolvDataset(val_encodings, y_val.values)\n",
    "test_dataset = FreeSolvDataset(test_encodings, y_test.values)\n",
    "\n",
    "# Define training arguments with early stopping and save the best model\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=100,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    ")\n",
    "\n",
    "# Define a custom compute_metrics function for regression\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions.squeeze()\n",
    "    mse = mean_squared_error(labels, predictions)\n",
    "    r2 = r2_score(labels, predictions)\n",
    "    return {\"mse\": mse, \"r2\": r2}\n",
    "\n",
    "# Create the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = trainer.create_optimizer()\n",
    "\n",
    "# Add a linear schedule with warmup\n",
    "num_training_steps = len(train_dataset) // training_args.per_device_train_batch_size * training_args.num_train_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# Assign the scheduler to the trainer\n",
    "trainer.lr_scheduler = scheduler\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "print(test_results)\n",
    "\n",
    "# Predict on the test set\n",
    "predictions = trainer.predict(test_dataset).predictions.squeeze()\n",
    "\n",
    "# Plot predictions vs true values for test set\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, predictions, alpha=0.6, color='green')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.title('Test Set: True Values vs Predictions')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fd1a49",
   "metadata": {},
   "source": [
    "Check if anything went wrong here then add SWA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a3bc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datamol as dm\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "\n",
    "# Load the ChemBERTa model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\", num_labels=1)\n",
    "\n",
    "# Load the data\n",
    "df = dm.data.freesolv()\n",
    "X, y = df[\"smiles\"], df[\"expt\"]\n",
    "\n",
    "# Tokenize the SMILES strings\n",
    "def tokenize_function(smiles):\n",
    "    return tokenizer(smiles, padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Convert the tokenized data to the format required by Hugging Face\n",
    "class FreeSolvDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Data augmentation: Randomize SMILES strings\n",
    "new_X, new_y = [], []\n",
    "listx = X_train.tolist()\n",
    "listy = y_train.tolist()\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    for j in range(100):\n",
    "        new_X.append(dm.randomize_smiles(listx[i]))  # Assume dm.randomize_smiles is a valid function\n",
    "        new_y.append(listy[i])\n",
    "\n",
    "X_train = pd.DataFrame(new_X).squeeze()\n",
    "y_train = pd.DataFrame(new_y).squeeze()\n",
    "\n",
    "# Apply the tokenization and assign the results back to the variables\n",
    "X_train = X_train.apply(tokenize_function)\n",
    "X_val = X_val.apply(tokenize_function)\n",
    "X_test = X_test.apply(tokenize_function)\n",
    "\n",
    "# Convert the tokenized data to tensors\n",
    "def convert_to_tensor(tokenized_data):\n",
    "    return {\n",
    "        'input_ids': torch.tensor([x['input_ids'] for x in tokenized_data]),\n",
    "        'attention_mask': torch.tensor([x['attention_mask'] for x in tokenized_data])\n",
    "    }\n",
    "\n",
    "train_encodings = convert_to_tensor(X_train)\n",
    "val_encodings = convert_to_tensor(X_val)\n",
    "test_encodings = convert_to_tensor(X_test)\n",
    "\n",
    "# Create the datasets\n",
    "train_dataset = FreeSolvDataset(train_encodings, y_train.values)\n",
    "val_dataset = FreeSolvDataset(val_encodings, y_val.values)\n",
    "test_dataset = FreeSolvDataset(test_encodings, y_test.values)\n",
    "\n",
    "# Define training arguments with early stopping and save the best model\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=100,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    ")\n",
    "\n",
    "# Define a custom compute_metrics function for regression\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions.squeeze()\n",
    "    mse = mean_squared_error(labels, predictions)\n",
    "    r2 = r2_score(labels, predictions)\n",
    "    return {\"mse\": mse, \"r2\": r2}\n",
    "\n",
    "# Create the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")\n",
    "\n",
    "# Create the optimizer and scheduler\n",
    "trainer.create_optimizer_and_scheduler(num_training_steps=len(train_dataset) // training_args.per_device_train_batch_size * training_args.num_train_epochs)\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "print(test_results)\n",
    "\n",
    "# Predict on the test set\n",
    "predictions = trainer.predict(test_dataset).predictions.squeeze()\n",
    "\n",
    "# Plot predictions vs true values for test set\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, predictions, alpha=0.6, color='green')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.title('Test Set: True Values vs Predictions')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3d5deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datamol as dm\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "import torch\n",
    "from torch.optim.swa_utils import AveragedModel, SWALR, update_bn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the ChemBERTa model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\", num_labels=1)\n",
    "\n",
    "# Load the data\n",
    "df = dm.data.freesolv()\n",
    "X, y = df[\"smiles\"], df[\"expt\"]\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Augment the training data with randomized SMILES strings\n",
    "new_X, new_y = [], []\n",
    "listx = X_train.tolist()\n",
    "listy = y_train.tolist() \n",
    "for i in range(len(X_train)):\n",
    "    for j in range(100):\n",
    "        new_X.append(sme.randomize_smiles(listx[i]))\n",
    "        new_y.append(listy[i])\n",
    "\n",
    "# Convert augmented data to DataFrame and Series\n",
    "X_train = pd.DataFrame(new_X).squeeze()\n",
    "y_train = pd.DataFrame(new_y).squeeze()\n",
    "\n",
    "# Tokenize the datasets\n",
    "train_encodings = tokenizer(list(X_train), padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "val_encodings = tokenizer(list(X_val), padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "test_encodings = tokenizer(list(X_test), padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "# Create the datasets\n",
    "class FreeSolvDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.float).clone().detach()\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = FreeSolvDataset(train_encodings, y_train.values)\n",
    "val_dataset = FreeSolvDataset(val_encodings, y_val.values)\n",
    "test_dataset = FreeSolvDataset(test_encodings, y_test.values)\n",
    "\n",
    "# Define training arguments with early stopping and save the best model\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    num_train_epochs=1000,\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    ")\n",
    "\n",
    "# Define a custom compute_metrics function for regression\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions.squeeze()\n",
    "    mse = mean_squared_error(labels, predictions)\n",
    "    r2 = r2_score(labels, predictions)\n",
    "    return {\"mse\": mse, \"r2\": r2}\n",
    "\n",
    "# Create the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = trainer.create_optimizer()\n",
    "\n",
    "# Custom learning rate scheduler\n",
    "class CustomLRScheduler(torch.optim.lr_scheduler.LambdaLR):\n",
    "    def __init__(self, optimizer, total_epochs, decay_epochs):\n",
    "        self.total_epochs = total_epochs\n",
    "        self.decay_epochs = decay_epochs\n",
    "        super().__init__(optimizer, self.lr_lambda)\n",
    "\n",
    "    def lr_lambda(self, epoch):\n",
    "        if epoch < self.decay_epochs:\n",
    "            return 1 - (epoch / self.decay_epochs) * 0.5  # Decay by a factor of 2\n",
    "        else:\n",
    "            return 0.5  # Remain flat\n",
    "\n",
    "# Initialize the custom scheduler\n",
    "scheduler = CustomLRScheduler(optimizer, total_epochs=training_args.num_train_epochs, decay_epochs=10)\n",
    "\n",
    "# Assign the scheduler to the trainer\n",
    "trainer.lr_scheduler = scheduler\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Apply SWA after epoch 10\n",
    "swa_model = AveragedModel(model)\n",
    "swa_start = 10\n",
    "swa_scheduler = SWALR(optimizer, swa_lr=1e-5)\n",
    "\n",
    "for epoch in range(training_args.num_train_epochs):\n",
    "    trainer.train()\n",
    "    if epoch >= swa_start:\n",
    "        swa_model.update_parameters(model)\n",
    "        swa_scheduler.step()\n",
    "\n",
    "# Update the batch norm statistics for SWA model\n",
    "update_bn(trainer.get_train_dataloader(), swa_model)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "print(test_results)\n",
    "\n",
    "# Predict on the test set\n",
    "predictions = trainer.predict(test_dataset).predictions.squeeze()\n",
    "\n",
    "# Plot predictions vs true values for test set\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, predictions, alpha=0.6, color='green')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.title('Test Set: True Values vs Predictions')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02112c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datamol as dm\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.swa_utils import AveragedModel, SWALR, update_bn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the ChemBERTa model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\", num_labels=1)\n",
    "\n",
    "# Load the data\n",
    "print(\"Loading data...\")\n",
    "df = dm.data.freesolv()\n",
    "X, y = df[\"smiles\"], df[\"expt\"]\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "print(\"Splitting data into train, validation, and test sets...\")\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Augment the training data with randomized SMILES strings\n",
    "print(\"Augmenting training data...\")\n",
    "new_X, new_y = [], []\n",
    "listx = X_train.tolist()\n",
    "listy = y_train.tolist()\n",
    "for i in range(len(X_train)):\n",
    "    for j in range(100):\n",
    "        new_X.append(listx[i])\n",
    "        new_y.append(listy[i])\n",
    "\n",
    "# Convert augmented data to DataFrame and Series\n",
    "X_train = pd.DataFrame(new_X).squeeze()\n",
    "y_train = pd.DataFrame(new_y).squeeze()\n",
    "\n",
    "# Tokenize the datasets\n",
    "print(\"Tokenizing datasets...\")\n",
    "train_encodings = tokenizer(list(X_train), padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "val_encodings = tokenizer(list(X_val), padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "test_encodings = tokenizer(list(X_test), padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "# Create the datasets\n",
    "class FreeSolvDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.float).clone().detach()\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "print(\"Creating datasets...\")\n",
    "train_dataset = FreeSolvDataset(train_encodings, y_train.values)\n",
    "val_dataset = FreeSolvDataset(val_encodings, y_val.values)\n",
    "test_dataset = FreeSolvDataset(test_encodings, y_test.values)\n",
    "\n",
    "# Create DataLoader for batching\n",
    "print(\"Creating DataLoader for batching...\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128)\n",
    "\n",
    "# Create the optimizer and learning rate scheduler\n",
    "print(\"Creating optimizer and learning rate scheduler...\")\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5, weight_decay=0.01)\n",
    "\n",
    "# Custom learning rate scheduler function\n",
    "def lr_lambda(epoch):\n",
    "    if epoch < 10:\n",
    "        return 1 - (epoch / 10) * 0.5  # Decay by a factor of 2\n",
    "    else:\n",
    "        return 0.5  # Remain flat\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# Initialize SWA\n",
    "print(\"Initializing SWA...\")\n",
    "swa_model = AveragedModel(model)\n",
    "swa_start = 10\n",
    "swa_scheduler = SWALR(optimizer, swa_lr=1e-5)\n",
    "\n",
    "# Early stopping variables\n",
    "best_val_loss = float('inf')\n",
    "best_swa_eval_loss = float('inf')\n",
    "patience = 5\n",
    "epochs_no_improve = 0\n",
    "best_model_state = None\n",
    "\n",
    "# Training loop\n",
    "print(\"Starting training loop...\")\n",
    "for epoch in range(1000):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\"):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1} - Training Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    for batch in tqdm(val_loader, desc=f\"Validation Epoch {epoch + 1}\"):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            eval_loss += loss.item()\n",
    "    \n",
    "    eval_loss /= len(val_loader)\n",
    "    print(f\"Epoch {epoch + 1} - Validation Loss: {eval_loss}\")\n",
    "\n",
    "    swa_eval_loss = None\n",
    "    if epoch >= swa_start:\n",
    "        swa_model.update_parameters(model)\n",
    "        swa_scheduler.step()\n",
    "        update_bn(train_loader, swa_model)\n",
    "        swa_eval_loss = 0\n",
    "        for batch in tqdm(val_loader, desc=f\"SWA Validation Epoch {epoch + 1}\"):\n",
    "            with torch.no_grad():\n",
    "                outputs = swa_model(**batch)\n",
    "                loss = outputs.loss\n",
    "                swa_eval_loss += loss.item()\n",
    "        \n",
    "        swa_eval_loss /= len(val_loader)\n",
    "        print(f\"SWA Model Evaluation after epoch {epoch + 1} - Validation Loss: {swa_eval_loss}\")\n",
    "\n",
    "    # Check for early stopping\n",
    "    if eval_loss < best_val_loss or (swa_eval_loss is not None and swa_eval_loss < best_swa_eval_loss):\n",
    "        if eval_loss < best_val_loss:\n",
    "            best_val_loss = eval_loss\n",
    "            best_model_state = model.state_dict()\n",
    "        if swa_eval_loss is not None and swa_eval_loss < best_swa_eval_loss:\n",
    "            best_swa_eval_loss = swa_eval_loss\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(\"Early stopping due to no improvement in validation loss or SWA validation loss.\")\n",
    "        break\n",
    "\n",
    "# Load the best model state\n",
    "print(\"Loading the best model state...\")\n",
    "model.load_state_dict(best_model_state)\n",
    "\n",
    "# Final SWA model evaluation\n",
    "print(\"Evaluating final SWA model...\")\n",
    "update_bn(train_loader, swa_model)\n",
    "model = swa_model\n",
    "test_loss = 0\n",
    "for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        test_loss += loss.item()\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "print(f\"Final Test Loss: {test_loss}\")\n",
    "\n",
    "# Predictions on the test set\n",
    "print(\"Generating predictions on the test set...\")\n",
    "model.eval()\n",
    "predictions = []\n",
    "for batch in tqdm(test_loader, desc=\"Predicting\"):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "        preds = outputs.logits.squeeze().tolist()\n",
    "        predictions.extend(preds)\n",
    "\n",
    "# Plot predictions vs true values for test set\n",
    "print(\"Plotting predictions vs true values...\")\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, predictions, alpha=0.6, color='green')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.title('Test Set: True Values vs Predictions')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740e6f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datamol as dm\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.swa_utils import AveragedModel, SWALR, update_bn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the ChemBERTa model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\", num_labels=1)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Load the data\n",
    "print(\"Loading data...\")\n",
    "df = dm.data.freesolv()\n",
    "X, y = df[\"smiles\"], df[\"expt\"]\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "print(\"Splitting data into train, validation, and test sets...\")\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Augment the training data with randomized SMILES strings\n",
    "print(\"Augmenting training data...\")\n",
    "new_X, new_y = [], []\n",
    "listx = X_train.tolist()\n",
    "listy = y_train.tolist()\n",
    "for i in range(len(X_train)):\n",
    "    for j in range(100):\n",
    "        new_X.append(sme.randomize_smiles(listx[i]))\n",
    "        new_y.append(listy[i])\n",
    "\n",
    "# Convert augmented data to DataFrame and Series\n",
    "X_train = pd.DataFrame(new_X).squeeze()\n",
    "y_train = pd.DataFrame(new_y).squeeze()\n",
    "\n",
    "# Tokenize the datasets\n",
    "print(\"Tokenizing datasets...\")\n",
    "train_encodings = tokenizer(list(X_train), padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "val_encodings = tokenizer(list(X_val), padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "test_encodings = tokenizer(list(X_test), padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "# Create the datasets\n",
    "class FreeSolvDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.float).clone().detach()\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "print(\"Creating datasets...\")\n",
    "train_dataset = FreeSolvDataset(train_encodings, y_train.values)\n",
    "val_dataset = FreeSolvDataset(val_encodings, y_val.values)\n",
    "test_dataset = FreeSolvDataset(test_encodings, y_test.values)\n",
    "\n",
    "# Create DataLoader for batching\n",
    "print(\"Creating DataLoader for batching...\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1024)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1024)\n",
    "\n",
    "# Create the optimizer and learning rate scheduler\n",
    "print(\"Creating optimizer and learning rate scheduler...\")\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=0.03)\n",
    "\n",
    "# Custom learning rate scheduler function\n",
    "def lr_lambda(epoch):\n",
    "    if epoch < 10:\n",
    "        return 1 - (epoch / 10) * 0.5  # Decay by a factor of 2\n",
    "    else:\n",
    "        return 0.5  # Remain flat\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# Initialize SWA\n",
    "print(\"Initializing SWA...\")\n",
    "swa_model = AveragedModel(model)\n",
    "swa_start = 10\n",
    "swa_scheduler = SWALR(optimizer, swa_lr=1e-5)\n",
    "\n",
    "# Early stopping variables\n",
    "best_val_loss = float('inf')\n",
    "best_swa_eval_loss = float('inf')\n",
    "patience = 5\n",
    "epochs_no_improve = 0\n",
    "best_model_state = None\n",
    "best_swa_model_state = None\n",
    "\n",
    "# Training loop\n",
    "print(\"Starting training loop...\")\n",
    "for epoch in range(1000):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\"):\n",
    "        optimizer.zero_grad()\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1} - Training Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    for batch in tqdm(val_loader, desc=f\"Validation Epoch {epoch + 1}\"):\n",
    "        with torch.no_grad():\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(**inputs, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            eval_loss += loss.item()\n",
    "    \n",
    "    eval_loss /= len(val_loader)\n",
    "    print(f\"Epoch {epoch + 1} - Validation Loss: {eval_loss}\")\n",
    "\n",
    "    swa_eval_loss = None\n",
    "    if epoch >= swa_start:\n",
    "        swa_model.update_parameters(model)\n",
    "        swa_scheduler.step()\n",
    "        update_bn(train_loader, swa_model)\n",
    "        swa_eval_loss = 0\n",
    "        for batch in tqdm(val_loader, desc=f\"SWA Validation Epoch {epoch + 1}\"):\n",
    "            with torch.no_grad():\n",
    "                inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "                labels = batch['labels'].to(device)\n",
    "                outputs = swa_model(**inputs, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                swa_eval_loss += loss.item()\n",
    "        \n",
    "        swa_eval_loss /= len(val_loader)\n",
    "        print(f\"SWA Model Evaluation after epoch {epoch + 1} - Validation Loss: {swa_eval_loss}\")\n",
    "\n",
    "        # Check for early stopping and best model state after SWA has started\n",
    "        if swa_eval_loss < best_swa_eval_loss:\n",
    "            best_swa_eval_loss = swa_eval_loss\n",
    "            best_swa_model_state = swa_model.state_dict()\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"Early stopping due to no improvement in SWA validation loss.\")\n",
    "            break\n",
    "\n",
    "    # Check for best model state based on validation loss\n",
    "    if eval_loss < best_val_loss:\n",
    "        best_val_loss = eval_loss\n",
    "        best_model_state = model.state_dict()\n",
    "\n",
    "# Load the best model state\n",
    "if best_swa_model_state is not None and best_swa_eval_loss < best_val_loss:\n",
    "    print(\"Loading the best SWA model state...\")\n",
    "    swa_model.load_state_dict(best_swa_model_state)\n",
    "    model = swa_model\n",
    "else:\n",
    "    print(\"Loading the best model state...\")\n",
    "    model.load_state_dict(best_model_state)\n",
    "\n",
    "# Final SWA model evaluation\n",
    "print(\"Evaluating final SWA model...\")\n",
    "update_bn(train_loader, swa_model)\n",
    "model = swa_model\n",
    "test_loss = 0\n",
    "for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "    with torch.no_grad():\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        test_loss += loss.item()\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "print(f\"Final Test Loss: {test_loss}\")\n",
    "\n",
    "# Predictions on the test set\n",
    "print(\"Generating predictions on the test set...\")\n",
    "model.eval()\n",
    "predictions = []\n",
    "for batch in tqdm(test_loader, desc=\"Predicting\"):\n",
    "    with torch.no_grad():\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "        outputs = model(**inputs)\n",
    "        preds = outputs.logits.squeeze().tolist()\n",
    "        predictions.extend(preds)\n",
    "\n",
    "# Plot predictions vs true values for test set\n",
    "print(\"Plotting predictions vs true values...\")\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, predictions, alpha=0.6, color='green')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.title('Test Set: True Values vs Predictions')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8232a067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model checkpoint with test loss to 2 decimal places\n",
    "checkpoint_path = \"model_checkpoint_{:.2f}.pth\".format(test_loss)\n",
    "torch.save(model.state_dict(), checkpoint_path)\n",
    "print(f\"Model checkpoint saved to {checkpoint_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88832b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(checkpoint_path))\n",
    "model.eval()\n",
    "\n",
    "# Define a function to tokenize input and get prediction\n",
    "def predict(smiles_string):\n",
    "    # Tokenize the input SMILES string\n",
    "    inputs = tokenizer(smiles_string, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    \n",
    "    # Move inputs to the appropriate device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Get the model prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        prediction = outputs.logits.squeeze().item()\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "# Example usage\n",
    "smiles_string = \"CCO\"  # Replace this with your SMILES string\n",
    "prediction = predict(smiles_string)\n",
    "print(f\"Prediction for {smiles_string}: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf3c498",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc23369c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max([len(s) for s in df[\"smiles\"].tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56934dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_list = [(len(s),s) for s in df[\"smiles\"].tolist()]\n",
    "check_list.sort(key=lambda x:x[0],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05846883",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(check_list[0][1], return_tensors=\"pt\", padding=True, truncation=True, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67052e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(check_list[0][1], return_tensors=\"pt\", padding=True, truncation=True, max_length=128)['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455dcfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(check_list[0][1], return_tensors=\"pt\", padding=True, truncation=True, max_length=512)['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de3f5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import Adam\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.optim.swa_utils import AveragedModel, SWALR, update_bn\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the ChemBERTa model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\", num_labels=1)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Load the data\n",
    "print(\"Loading data...\")\n",
    "df = dm.data.freesolv()\n",
    "X, y = df[\"smiles\"], df[\"expt\"]\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "print(\"Splitting data into train, validation, and test sets...\")\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Augment the training data with randomized SMILES strings\n",
    "print(\"Augmenting training data...\")\n",
    "new_X, new_y = [], []\n",
    "listx = X_train.tolist()\n",
    "listy = y_train.tolist()\n",
    "for i in range(len(X_train)):\n",
    "    for j in range(100):\n",
    "        new_X.append(sme.randomize_smiles(listx[i]))\n",
    "        new_y.append(listy[i])\n",
    "\n",
    "# Convert augmented data to DataFrame and Series\n",
    "X_train = pd.DataFrame(new_X).squeeze()\n",
    "y_train = pd.DataFrame(new_y).squeeze()\n",
    "\n",
    "# Tokenize the datasets\n",
    "print(\"Tokenizing datasets...\")\n",
    "train_encodings = tokenizer(list(X_train), padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "val_encodings = tokenizer(list(X_val), padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "test_encodings = tokenizer(list(X_test), padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "# Convert labels to tensors\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "# Create TensorDataset instances\n",
    "print(\"Creating datasets...\")\n",
    "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], y_train_tensor)\n",
    "val_dataset = TensorDataset(val_encodings['input_ids'], val_encodings['attention_mask'], y_val_tensor)\n",
    "test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], y_test_tensor)\n",
    "\n",
    "# Create DataLoader for batching\n",
    "print(\"Creating DataLoader for batching...\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1024)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1024)\n",
    "\n",
    "# Optimizer, scheduler and SWA setup\n",
    "optimizer = Adam(model.parameters(), lr=1e-5)\n",
    "total_steps = len(train_loader) * 1000\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "swa_model = AveragedModel(model)\n",
    "swa_start = 5  # Epoch to start SWA\n",
    "swa_scheduler = SWALR(optimizer, swa_lr=1e-5)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_swa_eval_loss = float('inf')\n",
    "best_model_state = None\n",
    "best_swa_model_state = None\n",
    "patience = 10\n",
    "epochs_no_improve = 0\n",
    "\n",
    "# Training loop\n",
    "print(\"Starting training loop...\")\n",
    "for epoch in range(1000):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\"):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids, attention_mask, labels = (item.to(device) for item in batch)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1} - Training Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    for batch in tqdm(val_loader, desc=f\"Validation Epoch {epoch + 1}\"):\n",
    "        with torch.no_grad():\n",
    "            input_ids, attention_mask, labels = (item.to(device) for item in batch)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            eval_loss += loss.item()\n",
    "    \n",
    "    eval_loss /= len(val_loader)\n",
    "    print(f\"Epoch {epoch + 1} - Validation Loss: {eval_loss}\")\n",
    "\n",
    "    swa_eval_loss = None\n",
    "    if epoch >= swa_start:\n",
    "        swa_model.update_parameters(model)\n",
    "        swa_scheduler.step()\n",
    "        update_bn(train_loader, swa_model)\n",
    "        swa_eval_loss = 0\n",
    "        for batch in tqdm(val_loader, desc=f\"SWA Validation Epoch {epoch + 1}\"):\n",
    "            with torch.no_grad():\n",
    "                input_ids, attention_mask, labels = (item.to(device) for item in batch)\n",
    "                outputs = swa_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                swa_eval_loss += loss.item()\n",
    "        \n",
    "        swa_eval_loss /= len(val_loader)\n",
    "        print(f\"SWA Model Evaluation after epoch {epoch + 1} - Validation Loss: {swa_eval_loss}\")\n",
    "\n",
    "        # Check for early stopping and best model state after SWA has started\n",
    "        if swa_eval_loss < best_swa_eval_loss:\n",
    "            best_swa_eval_loss = swa_eval_loss\n",
    "            best_swa_model_state = swa_model.state_dict()\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"Early stopping due to no improvement in SWA validation loss.\")\n",
    "            break\n",
    "\n",
    "    # Check for best model state based on validation loss\n",
    "    if eval_loss < best_val_loss:\n",
    "        best_val_loss = eval_loss\n",
    "        best_model_state = model.state_dict()\n",
    "\n",
    "# Load the best model state\n",
    "if best_swa_model_state is not None and best_swa_eval_loss < best_val_loss:\n",
    "    print(\"Loading the best SWA model state...\")\n",
    "    swa_model.load_state_dict(best_swa_model_state)\n",
    "    model = swa_model\n",
    "else:\n",
    "    print(\"Loading the best model state...\")\n",
    "    model.load_state_dict(best_model_state)\n",
    "\n",
    "# Final SWA model evaluation\n",
    "print(\"Evaluating final SWA model...\")\n",
    "update_bn(train_loader, swa_model)\n",
    "model = swa_model\n",
    "test_loss = 0\n",
    "for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "    with torch.no_grad():\n",
    "        input_ids, attention_mask, labels = (item.to(device) for item in batch)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        test_loss += loss.item()\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "print(f\"Final Test Loss: {test_loss}\")\n",
    "\n",
    "# Predictions on the test set\n",
    "print(\"Generating predictions on the test set...\")\n",
    "model.eval()\n",
    "predictions = []\n",
    "for batch in tqdm(test_loader, desc=\"Predicting\"):\n",
    "    with torch.no_grad():\n",
    "        input_ids, attention_mask = (item.to(device) for item in batch[:2])\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        preds = outputs.logits.squeeze().tolist()\n",
    "        predictions.extend(preds)\n",
    "\n",
    "# Plot predictions vs true values for test set\n",
    "print(\"Plotting predictions vs true values...\")\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, predictions, alpha=0.6, color='green')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.title('Test Set: True Values vs Predictions')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a049a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
